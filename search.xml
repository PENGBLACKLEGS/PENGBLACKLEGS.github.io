<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[图形验证码的识别]]></title>
    <url>%2F2018%2F08%2F02%2F%E5%9B%BE%E5%BD%A2%E9%AA%8C%E8%AF%81%E7%A0%81%E7%9A%84%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[图形验证码识别利用OCR技术可以对图形验证码进行识别，要先准备库tesserocr，如果安装出错，那就安装pytesseract库，因为tesserocr主要用于Linux系统，在win系统中有太多的不兼容，那么我们选用谷歌的pytesseract一样可以达到目的 验证码处理12345import pytesseractfrom PIL import Imageimage = Image.open('code.jpg')print(pytesseract.image_to_string(image)) 如果直接利用第三方库肯定识别会有偏差，这是因为验证码旁边多余的线条干扰了图片的识别 图片处理那么我们就必须对图片进行处理，如转灰度、二值化等利用Image对象的convert()的方法参数传入L，即可将图像转化为灰度图像，代码如下： 12image = image.convert('L')image.show() 传入1，对图片进行二值化，代码如下：12image = image.convert('1')image.show() 同时我们还可以指出二值化的阈值，上面采用的默认127阈值，我们不能直接转化原图，要将原图转化为灰度图像，而后在指定二值化阈值，代码如下： 1234567891011image = image.convert('L')threshold = 80table = []for i in range(256): if i &lt; threshold: table.append(0) else: table.append(1)image = image.point(table, '1')image.show() 综合代码如下：12345678910111213141516171819import tesserocrfrom PIL import Imageimage = Image.open('code.jpg')image = image.convert('L')threshold = 127table = []for i in range(256): if i &lt; threshold: table.append(0) else: table.append(1)image = image.point(table, '1')image.show()result = tesserocr.image_to_text(image)print(result) 输出：GTVM]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Network Crawler</tag>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用XPath的特性简单抓取豆瓣电影]]></title>
    <url>%2F2018%2F08%2F01%2F%E5%88%A9%E7%94%A8XPath%E7%9A%84%E7%89%B9%E6%80%A7%E7%AE%80%E5%8D%95%E6%8A%93%E5%8F%96%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%2F</url>
    <content type="text"><![CDATA[今天突然想回顾一下python解析库XPath，利用Xpath的特性可以比正则表达式更快捷的获取网页中的HTML节点，更快的获取网页中的数据。 实例如下:利用requests模拟用户1234567891011import requestsfrom lxml import etreeheaders=&#123; 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36', 'Referer':'https://movie.douban.com/'&#125;url='https://movie.douban.com/cinema/nowplaying/zhengzhou/'response=requests.get(url,headers=headers)text=response.text 利用Xpath的特性获取HTML节点内容1234567891011121314151617181920212223html=etree.HTML(text)ul=html.xpath("//ul[@class='lists']")[0]lis=ul.xpath("./li")for li in lis: title=li.xpath("@data-title")[0] score = li.xpath("@data-score")[0] duration = li.xpath("@data-duration")[0] region=li.xpath("@data-region")[0] director=li.xpath("@data-director")[0] actors=li.xpath("@data-actors")[0] thumbnail=li.xpath(".//img/@src")[0] movies = [] movie=&#123; 'title': title, 'score':score, 'duration':duration, 'region':region, 'director':director, 'actors':actors, 'thumbnail':thumbnail &#125; movies.append(movie)print(movies) 是不是觉得比正则表达式简单许多！！！ 网页HTML节点内容图片]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Network Crawler</tag>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分析Ajax爬取今日头条街拍美图]]></title>
    <url>%2F2018%2F07%2F31%2F%E5%88%86%E6%9E%90Ajax%E7%88%AC%E5%8F%96%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1%E8%A1%97%E6%8B%8D%E7%BE%8E%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[今天初学了Ajax，Ajax技术可以利用JavaScript在网页不被刷新的情况下与服务器交换数据并更新网页的部分技术，于是乎，利用Ajax的这一特性，决定去爬取今日头条的街拍图片并且实现自动分类汇总。 实战演练实现方法get_page()来加载单个的Ajax的请求过程123456789101112131415161718import requestsfrom urllib.parse import urlencodedef get_page(offert): params=&#123; 'offset':offset, 'format':'json', 'keyword':'街拍', 'autoload':'ture', 'count':'20', 'cur_tab':'1' &#125; url='https://www.toutiao.com/search_content/?'+ urlencode(params) try: response=requests.get(url) if response.status_code==200: return response.json() except requests.ConnectionError: return None 这里我们通过网页开发者模式可以看出URL的基本构造，唯一不变的是参数offset，所以我们将他作为参数传递，我们使用urlencode()的方法来实现网页的GET请求，然后用requests实现整个网页的GET请求，如果连接的状态码为200，那么返回为用函数response的json()将结果转化为JSON格式内容。 获取图片并且截获主题标题12345678910def get_image(json): if json.get('data'): for item in json.get('title'): title=item.get('title') images=item.get('image_detail') for image in images: yield &#123; 'image':image.get('url'), 'title':title &#125; 实现一个解析方法：提取每条数据中的”imag_etail”字段中的每一张图片链接，将图片链接和图片所属的标题一并返回，此时可以构造一个生成器。 实现保存图片的方法1234567891011121314151617import osfrom hashlib import md5def save_image(item): if not os.path.exists(item.get('title')): os.mkdir(item.get('title')) try: response=requests.get(item.get('image')) if response.status_code==200: file_path = '&#123;0&#125;/&#123;1&#125;.&#123;2&#125;'.format(item.get('title'),md5(response.content).hexdigest(),'jpg') if not os.path.exists(file_path): with open(file_path,'wb') as f: f.write(response.content) else: print('Already Downloaded',file_path) except requests.ConnectionError: print('Failed to Save Image') 用save_image()函数实现图片的保存，item不过是前面get_image函数返回的一个字典。在该方法中，首先根据item的title来创建一个文件夹，然后请求这个图片链接，获取图片的二进制数据，以二进制的形式写入文件。图片的名称可以使用可以使用其内容的Md5的值，这样可以去除反复 实现多进程下载12345678910111213def main(offset): json = get_page(offset) for item in get_images(json): print(item) save_image(item)if __name__ == '__main__': pool = Pool() groups = ([x * 20 for x in range(GROUP_START, GROUP_END + 1)]) pool.map(main, groups) pool.close() pool.join() 我们只需要构造一个offset数组，遍历offset，提取图片链接并下载，在这里也实现了起始页数和终止页数，还利用了多进程池，调用map()的方法实现多进程下载]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Network Crawler</tag>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用第三方解析库爬取知乎推荐页面并采用txt文本储存]]></title>
    <url>%2F2018%2F07%2F31%2F%E5%88%A9%E7%94%A8%E7%AC%AC%E4%B8%89%E6%96%B9%E8%A7%A3%E6%9E%90%E5%BA%93%E7%88%AC%E5%8F%96%E7%9F%A5%E4%B9%8E%E6%8E%A8%E8%8D%90%E9%A1%B5%E9%9D%A2%E5%B9%B6%E9%87%87%E7%94%A8txt%E6%96%87%E6%9C%AC%E5%82%A8%E5%AD%98%2F</url>
    <content type="text"><![CDATA[采用第三方库快捷爬取知乎推荐页面源代码并解析：123456789101112131415161718import requests from pyquery import PyQuery as pqurl="https://www.zhihu.com/explore"headers =&#123; 'User-Agent':"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36"&#125; #虚拟用户登陆，防止封iphtml=requests.get(url,headers=headers).textdoc=pq(html) #初始化文本items=doc('.explore-tab .feed-item').items()for item in items: question=item.find("h2").text() author=item.find(".author-link-line").text() answer=pq(item.find('.content').html()).text() file=open('explore.txt','a',encoding='utf-8') file.write('\n'.join([question,author,answer])) file.write('\n'+'=' * 10 + '\n') file.close() 文本代码解析这里主要是为了演示文件保存的方式，因此省略requests库的异常处理应用，通过requests库提取知乎发现一栏的网页源代码，然后爬取知乎热门话题中的问题、回答者、答案全部提取出来，利用python中的open打开一个文件，write写入文件，最后用close关闭，内容也就保存在文本中了 效果图如下：]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Network Crawer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫实战（一）爬取猫眼top前100]]></title>
    <url>%2F2018%2F07%2F30%2F%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BCtop%E5%89%8D100%2F</url>
    <content type="text"><![CDATA[爬虫实战（一）利用python爬取猫眼电影Top前100代码及其解析：12345678910111213141516171819202122232425262728293031323334353637383940import requests#爬虫库import json#json数据格式库 from requests.exceptions import RequestException#requests异常 import re#正则表达式 import time #延迟函数 def get_one_page(url):#定义一个读取一个url并返回相应信息的函数try: headers=&#123; "User-Agent":'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36' &#125;#伪装浏览器 response=requests.get(url,headers=headers)#读取网页 if response.status_code==200:#判断是否读取成功 return response.text#返回读取的内容（html代码） return Noneexcept RequestException: return None def parse_one_page(html):#定义一个解析html代码的函数#编译成一个正则表达式对象pattern = re.compile(r'&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a' + '.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p&gt;' + '.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S) items = re.findall(pattern, html)#开始查找 for item in items:#遍历查找到的内容#使用关键字yield 类似于return 返回的是一个生成器对象 yield &#123;'index': item[0], 'image': item[1], 'title': item[2], 'actor': item[3].strip()[3:], 'time': item[4].strip()[5:], 'score': item[5] + item[6] &#125; def write_to_file(content):#将结果写到一个txt文档中 with open('result.txt', 'a', encoding='utf-8') as f: f.write(json.dumps(content, ensure_ascii=False) + '\n') def pic_download(url,title):#图片下载 r=requests.get(url) with open("pics/"+title+".jpg",'wb') as f: f.write(r.content) def main(offset):#打开需要爬取得所有网页，并进行爬取 url='http://maoyan.com/board/4?offset='+str(offset)#网页链接 html=get_one_page(url)#请求网页，获取html for item in parse_one_page(html):#遍历处理后的html结果 pic_download(item['image'], item['title'])#下载图片 write_to_file(item)#写入到文件中if __name__ == '__main__': for i in range(10): main(offset=i*10) time.sleep(1)#延迟1秒，避免反爬机制 效果图如下：]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Network Crawler</tag>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的a[::1]类型]]></title>
    <url>%2F2018%2F07%2F30%2FPython%E4%B8%AD%E7%9A%84%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[for value in rang(10)涉及的数字倒序输出： for value in rang(10)[::-1]涉及的数字倒序输出： 一、反转二、详解这个是python的slice notation的特殊用法。 a = [0,1,2,3,4,5,6,7,8,9] b = a[i:j] 表示复制a[i]到a[j-1]，以生成新的list对象 b = a[1:3] 那么，b的内容是 [1,2] 当i缺省时，默认为0，即 a[:3]相当于 a[0:3] 当j缺省时，默认为len(alist), 即a[1:]相当于a[1:10] 当i,j都缺省时，a[:]就相当于完整复制一份a了 b = a[i:j:s]这种格式呢，i,j与上面的一样，但s表示步进，缺省为1. 所以a[i:j:1]相当于a[i:j] 当s&lt;0时，i缺省时，默认为-1. j缺省时，默认为-len(a)-1所以a[::-1]相当于 a[-1:-len(a)-1:-1]，也就是从最后一个元素到第一个元素复制一遍。所以你看到一个倒序的东东。 如果还不理解，把我说的东西测试一遍，你就明白了]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>python Basic Grammar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中__name__的意义以及作用]]></title>
    <url>%2F2018%2F07%2F30%2Fpython%E4%B8%AD-name-%E7%9A%84%E6%84%8F%E4%B9%89%E4%BB%A5%E5%8F%8A%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[首先定义了一个test.py的文件，然后再定义一个函数，并在函数定义后直接运行： test.py def HaveFun(): if __name__ == &apos;__main__&apos;: print(&apos;I am in my domain,my name is %s&apos; % __name__) else: print(&apos;Someone else calls me!,my name is %s&apos; % __name__) HaveFun() 运行test.py结果： I am in my domain,my name is __main__ 然后继续创建一个main.py的文件，程序如下： main.py import test test.HaveFun() 执行main.py文件，结果如下： Someone else calls me!,my name is test Someone else calls me!,my name is test 这里打印了两次，第一次实在main.py在进行import test的时候，进行的打印，第二次才是test.HaveFun()中执行的打印，可以发现，这里的 name名称已经发成了变化，从之前的main变成了模块名称test。 总结：1、__name__这个系统变量显示了当前模块执行过程中的名称，如果当前程序运行在这个模块中，__name__ 的名称就是__main__如果不是，则为这个模块的名称。 2、__main__一般作为函数的入口，类似于C语言，尤其在大型工程中，常常有if __name__ == &quot;__main__&quot;:来表明整个工程开始运行的入口。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>python Basic Grammar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux中如何开启root]]></title>
    <url>%2F2018%2F07%2F30%2Flinux%E4%B8%ADroot%E6%A8%A1%E5%BC%8F%E5%BC%80%E5%90%AF%2F</url>
    <content type="text"><![CDATA[sudo passwd root, 为root输入新密码， 然后输入su root 回车后， 输入刚才新建的root密码， 就进入了root用户。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pycharm的快捷操作汇总]]></title>
    <url>%2F2018%2F07%2F30%2Fpycharm%E7%9A%84%E5%BF%AB%E6%8D%B7%E6%93%8D%E4%BD%9C%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[Alt+Enter 自动添加包 shift+O 自动建议代码补全 Ctrl+t SVN更新 Ctrl+k SVN提交 Ctrl + / 注释(取消注释)选择的行 Ctrl+Shift+F 高级查找 Ctrl+Enter 补全 Shift + Enter 开始新行 TAB Shift+TAB 缩进/取消缩进所选择的行 Ctrl + Alt + I 自动缩进行 Ctrl + Y 删除当前插入符所在的行 Ctrl + D 复制当前行、或者选择的块 Ctrl + Shift + J 合并行 Ctrl + Shift + V 从最近的缓存区里粘贴 Ctrl + Delete 删除到字符结尾 Ctrl + Backspace 删除到字符的开始 Ctrl + NumPad+/- 展开或者收缩代码块 Ctrl + Shift + NumPad+ 展开所有的代码块 Ctrl + Shift + NumPad- 收缩所有的代码块]]></content>
      <categories>
        <category>Python3</category>
        <category>Pycharm</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pycharm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python解析库XPath]]></title>
    <url>%2F2018%2F07%2F29%2FXpath%2F</url>
    <content type="text"><![CDATA[Python解析库（一）：使用XPath表达式 描述nodename 选取此节点的所有子节点 / 从当前节点选取直接子节点 // 从当前节点选取子孙节点 . 选取当前节点 . . 选取当前节点的父节点 @ 选取属性 例如： //title[@lang=’eng’] 这是一个XPath规则，他代表选取所有名称为title，同时属性为lang的值为ang的节点。 2.关于lxml库中的etree模块：先用etree模块申明一段HTML文本，etree模块可自动补全HTML文本， 然后调用tostring（）方法输出修正后的HTML文本，结果为bytes型 利用decode（）方法转化为str类型 3.所有节点：用//开头选取所有符合要求的节点，声明文本后，调用xpath（）方法，例如：xpath（//节点名称） 返回的为列表，其中的每一个元素都是一个Element对象，可选择中括号加索引的形式取出其中的一个对象 4.子节点：通过//和/来获取当前节点的子节点或者子孙节点； /用来获取直接子节点，例如result=html.xpath（//li/a） //用来获取子孙节点，例如result=html.xpath（//ul//a） 5.父节点：用. .来查找父节点 比如： from lxml import etree html = etree.parse(&apos;./text.html&apos; , etree.HTMLParser()) result = html.xpath(&apos;//a[@href=&quot;link4.html&quot;]/. ./@class&apos;] print(result) 当然，. .可以用parent::来替换 6.属性匹配：比如要选取class为item-0的li节点，可以这样实现： from lxml import etree html=etree.parse(&apos;./text.html&apos;,etree.HTMLParser()) result = html.xpath(&apos;//li[@class=&apos;items-0&apos;]&apos;) print(result) 7.文本获取：可以利用XPath中的text（）方法获取其中节点中的文本 选取相应的节点获取文本 from lxml import etree html = etree.parse(&apos;./text.html&apos; , etree.HTMLParser()) result = html.xpath(&apos;//li[@class=&quot;item-0&quot;]/a/text()&apos;) print(result) 利用//来实现选取结果 from lxml import etree html = etree.parse(&apos;./text.html&apos; , etree.HTMLParser()) result = html.xpath(&apos;//li[@class=&quot;item-0&quot;]/a/text()&apos;) print(result) 8.按序选择：在li节点的括号中传入数字1即可，这里的序号是以1开头，不是0 例如：result = html.xpath(&apos;//li[1]/a/text()&apos;) 在括号中传入last（）可知，获取的便是最后一个li节点 例如：result = html.xpath(&apos;//li[last()]/a/text()&apos;) 选取位置小于3的li节点，利用position（）函数实现 例如：result = html.xpath(&apos;//li[position()&lt;3]/a/text()&apos;) 获取倒数第三个li节点：用last()实现 例如：result = html.xpath(&apos;//li[last()-2]/a/text()&apos;) 9.节点轴的选取：节点的轴方法，可获取子元素，兄弟元素，父元素，祖先元素等 result = html.xpath(&apos;//li[1]/ancestor::*&apos;) #ancestor轴，用于获取所有祖先元素 result = html.xpath(&apos;//li[1]/ancestor::div&apos;)#加限定条件div result = html.xpath(&apos;//li[1]/child::a[href=&quot;link1.html&quot;]&apos;)#获取相应属性的子节点 result = html.xpath(&apos;//li[1]/descendant::span&apos;)#descendant获取所有的子孙节点，span是限定条件 result = html.xpath(&apos;//li[1]/following::*[2]&apos;)#following轴，获取当前节点之后的所有节点，但是这里加了索引限制 result = html.xpath(&apos;//li[1]/following-sibling::*&apos;)#follow-sibling轴，可以获取当前节点之后的所有同级节点]]></content>
      <categories>
        <category>Python3</category>
        <category>Network Crawler</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Network Crawler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F07%2F29%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
