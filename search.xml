<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[利用第三方解析库爬取知乎推荐页面并采用txt文本储存]]></title>
    <url>%2F2018%2F07%2F31%2F%E5%88%A9%E7%94%A8%E7%AC%AC%E4%B8%89%E6%96%B9%E8%A7%A3%E6%9E%90%E5%BA%93%E7%88%AC%E5%8F%96%E7%9F%A5%E4%B9%8E%E6%8E%A8%E8%8D%90%E9%A1%B5%E9%9D%A2%E5%B9%B6%E9%87%87%E7%94%A8txt%E6%96%87%E6%9C%AC%E5%82%A8%E5%AD%98%2F</url>
    <content type="text"><![CDATA[采用第三方库快捷爬取知乎推荐页面源代码并解析：123456789101112131415161718import requests from pyquery import PyQuery as pqurl="https://www.zhihu.com/explore"headers =&#123; 'User-Agent':"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36"&#125; #虚拟用户登陆，防止封iphtml=requests.get(url,headers=headers).textdoc=pq(html) #初始化文本items=doc('.explore-tab .feed-item').items()for item in items: question=item.find("h2").text() author=item.find(".author-link-line").text() answer=pq(item.find('.content').html()).text() file=open('explore.txt','a',encoding='utf-8') file.write('\n'.join([question,author,answer])) file.write('\n'+'=' * 10 + '\n') file.close() 文本代码解析这里主要是为了演示文件保存的方式，因此省略requests库的异常处理应用，通过requests库提取知乎发现一栏的网页源代码，然后爬取知乎热门话题中的问题、回答者、答案全部提取出来，利用python中的open打开一个文件，write写入文件，最后用close关闭，内容也就保存在文本中了 效果图如下：]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Network Crawer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫实战（一）爬取猫眼top前100]]></title>
    <url>%2F2018%2F07%2F30%2F%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BCtop%E5%89%8D100%2F</url>
    <content type="text"><![CDATA[爬虫实战（一）利用python爬取猫眼电影Top前100代码及其解析：12345678910111213141516171819202122232425262728293031323334353637383940import requests#爬虫库import json#json数据格式库 from requests.exceptions import RequestException#requests异常 import re#正则表达式 import time #延迟函数 def get_one_page(url):#定义一个读取一个url并返回相应信息的函数try: headers=&#123; "User-Agent":'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36' &#125;#伪装浏览器 response=requests.get(url,headers=headers)#读取网页 if response.status_code==200:#判断是否读取成功 return response.text#返回读取的内容（html代码） return Noneexcept RequestException: return None def parse_one_page(html):#定义一个解析html代码的函数#编译成一个正则表达式对象pattern = re.compile(r'&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a' + '.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p&gt;' + '.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S) items = re.findall(pattern, html)#开始查找 for item in items:#遍历查找到的内容#使用关键字yield 类似于return 返回的是一个生成器对象 yield &#123;'index': item[0], 'image': item[1], 'title': item[2], 'actor': item[3].strip()[3:], 'time': item[4].strip()[5:], 'score': item[5] + item[6] &#125; def write_to_file(content):#将结果写到一个txt文档中 with open('result.txt', 'a', encoding='utf-8') as f: f.write(json.dumps(content, ensure_ascii=False) + '\n') def pic_download(url,title):#图片下载 r=requests.get(url) with open("pics/"+title+".jpg",'wb') as f: f.write(r.content) def main(offset):#打开需要爬取得所有网页，并进行爬取 url='http://maoyan.com/board/4?offset='+str(offset)#网页链接 html=get_one_page(url)#请求网页，获取html for item in parse_one_page(html):#遍历处理后的html结果 pic_download(item['image'], item['title'])#下载图片 write_to_file(item)#写入到文件中if __name__ == '__main__': for i in range(10): main(offset=i*10) time.sleep(1)#延迟1秒，避免反爬机制 效果图如下：]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Network Crawler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的a[::1]类型]]></title>
    <url>%2F2018%2F07%2F30%2FPython%E4%B8%AD%E7%9A%84%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[for value in rang(10)涉及的数字倒序输出： for value in rang(10)[::-1]涉及的数字倒序输出： 一、反转二、详解这个是python的slice notation的特殊用法。 a = [0,1,2,3,4,5,6,7,8,9] b = a[i:j] 表示复制a[i]到a[j-1]，以生成新的list对象 b = a[1:3] 那么，b的内容是 [1,2] 当i缺省时，默认为0，即 a[:3]相当于 a[0:3] 当j缺省时，默认为len(alist), 即a[1:]相当于a[1:10] 当i,j都缺省时，a[:]就相当于完整复制一份a了 b = a[i:j:s]这种格式呢，i,j与上面的一样，但s表示步进，缺省为1. 所以a[i:j:1]相当于a[i:j] 当s&lt;0时，i缺省时，默认为-1. j缺省时，默认为-len(a)-1所以a[::-1]相当于 a[-1:-len(a)-1:-1]，也就是从最后一个元素到第一个元素复制一遍。所以你看到一个倒序的东东。 如果还不理解，把我说的东西测试一遍，你就明白了]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>python Basic Grammar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中__name__的意义以及作用]]></title>
    <url>%2F2018%2F07%2F30%2Fpython%E4%B8%AD-name-%E7%9A%84%E6%84%8F%E4%B9%89%E4%BB%A5%E5%8F%8A%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[首先定义了一个test.py的文件，然后再定义一个函数，并在函数定义后直接运行： test.py def HaveFun(): if __name__ == &apos;__main__&apos;: print(&apos;I am in my domain,my name is %s&apos; % __name__) else: print(&apos;Someone else calls me!,my name is %s&apos; % __name__) HaveFun() 运行test.py结果： I am in my domain,my name is __main__ 然后继续创建一个main.py的文件，程序如下： main.py import test test.HaveFun() 执行main.py文件，结果如下： Someone else calls me!,my name is test Someone else calls me!,my name is test 这里打印了两次，第一次实在main.py在进行import test的时候，进行的打印，第二次才是test.HaveFun()中执行的打印，可以发现，这里的 name名称已经发成了变化，从之前的main变成了模块名称test。 总结：1、__name__这个系统变量显示了当前模块执行过程中的名称，如果当前程序运行在这个模块中，__name__ 的名称就是__main__如果不是，则为这个模块的名称。 2、__main__一般作为函数的入口，类似于C语言，尤其在大型工程中，常常有if __name__ == &quot;__main__&quot;:来表明整个工程开始运行的入口。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>python Basic Grammar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux中如何开启root]]></title>
    <url>%2F2018%2F07%2F30%2Flinux%E4%B8%ADroot%E6%A8%A1%E5%BC%8F%E5%BC%80%E5%90%AF%2F</url>
    <content type="text"><![CDATA[sudo passwd root, 为root输入新密码， 然后输入su root 回车后， 输入刚才新建的root密码， 就进入了root用户。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pycharm的快捷操作汇总]]></title>
    <url>%2F2018%2F07%2F30%2Fpycharm%E7%9A%84%E5%BF%AB%E6%8D%B7%E6%93%8D%E4%BD%9C%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[Alt+Enter 自动添加包 shift+O 自动建议代码补全 Ctrl+t SVN更新 Ctrl+k SVN提交 Ctrl + / 注释(取消注释)选择的行 Ctrl+Shift+F 高级查找 Ctrl+Enter 补全 Shift + Enter 开始新行 TAB Shift+TAB 缩进/取消缩进所选择的行 Ctrl + Alt + I 自动缩进行 Ctrl + Y 删除当前插入符所在的行 Ctrl + D 复制当前行、或者选择的块 Ctrl + Shift + J 合并行 Ctrl + Shift + V 从最近的缓存区里粘贴 Ctrl + Delete 删除到字符结尾 Ctrl + Backspace 删除到字符的开始 Ctrl + NumPad+/- 展开或者收缩代码块 Ctrl + Shift + NumPad+ 展开所有的代码块 Ctrl + Shift + NumPad- 收缩所有的代码块]]></content>
      <categories>
        <category>Python3</category>
        <category>Pycharm</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pycharm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python解析库XPath]]></title>
    <url>%2F2018%2F07%2F29%2FXpath%2F</url>
    <content type="text"><![CDATA[Python解析库（一）：使用XPath表达式 描述nodename 选取此节点的所有子节点 / 从当前节点选取直接子节点 // 从当前节点选取子孙节点 . 选取当前节点 . . 选取当前节点的父节点 @ 选取属性 例如： //title[@lang=’eng’] 这是一个XPath规则，他代表选取所有名称为title，同时属性为lang的值为ang的节点。 2.关于lxml库中的etree模块：先用etree模块申明一段HTML文本，etree模块可自动补全HTML文本， 然后调用tostring（）方法输出修正后的HTML文本，结果为bytes型 利用decode（）方法转化为str类型 3.所有节点：用//开头选取所有符合要求的节点，声明文本后，调用xpath（）方法，例如：xpath（//节点名称） 返回的为列表，其中的每一个元素都是一个Element对象，可选择中括号加索引的形式取出其中的一个对象 4.子节点：通过//和/来获取当前节点的子节点或者子孙节点； /用来获取直接子节点，例如result=html.xpath（//li/a） //用来获取子孙节点，例如result=html.xpath（//ul//a） 5.父节点：用. .来查找父节点 比如： from lxml import etree html = etree.parse(&apos;./text.html&apos; , etree.HTMLParser()) result = html.xpath(&apos;//a[@href=&quot;link4.html&quot;]/. ./@class&apos;] print(result) 当然，. .可以用parent::来替换 6.属性匹配：比如要选取class为item-0的li节点，可以这样实现： from lxml import etree html=etree.parse(&apos;./text.html&apos;,etree.HTMLParser()) result = html.xpath(&apos;//li[@class=&apos;items-0&apos;]&apos;) print(result) 7.文本获取：可以利用XPath中的text（）方法获取其中节点中的文本 选取相应的节点获取文本 from lxml import etree html = etree.parse(&apos;./text.html&apos; , etree.HTMLParser()) result = html.xpath(&apos;//li[@class=&quot;item-0&quot;]/a/text()&apos;) print(result) 利用//来实现选取结果 from lxml import etree html = etree.parse(&apos;./text.html&apos; , etree.HTMLParser()) result = html.xpath(&apos;//li[@class=&quot;item-0&quot;]/a/text()&apos;) print(result) 8.按序选择：在li节点的括号中传入数字1即可，这里的序号是以1开头，不是0 例如：result = html.xpath(&apos;//li[1]/a/text()&apos;) 在括号中传入last（）可知，获取的便是最后一个li节点 例如：result = html.xpath(&apos;//li[last()]/a/text()&apos;) 选取位置小于3的li节点，利用position（）函数实现 例如：result = html.xpath(&apos;//li[position()&lt;3]/a/text()&apos;) 获取倒数第三个li节点：用last()实现 例如：result = html.xpath(&apos;//li[last()-2]/a/text()&apos;) 9.节点轴的选取：节点的轴方法，可获取子元素，兄弟元素，父元素，祖先元素等 result = html.xpath(&apos;//li[1]/ancestor::*&apos;) #ancestor轴，用于获取所有祖先元素 result = html.xpath(&apos;//li[1]/ancestor::div&apos;)#加限定条件div result = html.xpath(&apos;//li[1]/child::a[href=&quot;link1.html&quot;]&apos;)#获取相应属性的子节点 result = html.xpath(&apos;//li[1]/descendant::span&apos;)#descendant获取所有的子孙节点，span是限定条件 result = html.xpath(&apos;//li[1]/following::*[2]&apos;)#following轴，获取当前节点之后的所有节点，但是这里加了索引限制 result = html.xpath(&apos;//li[1]/following-sibling::*&apos;)#follow-sibling轴，可以获取当前节点之后的所有同级节点]]></content>
      <categories>
        <category>Python3</category>
        <category>Network Crawler</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Network Crawler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F07%2F29%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
