<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[GitHub导航栏的基本使用]]></title>
    <url>%2F2018%2F10%2F12%2FGitHub%E5%AF%BC%E8%88%AA%E6%A0%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[GitHub导航栏使用的基本概念仓库（Repository）​ 仓库的意思是说，如果你想在GitHub上开源一个项目，那就必须新建一个Repository，如果你的开源项目多了，你就会拥有多个Repositories。 收藏（Star）​ 仓库页面的star按钮，意思为收藏项目的人数，在GitHub上如果有100个star，都是非常的不容易。 复制克隆项目（Fork）​ 什么意思呢？就比如说你开源了一个项目，别人想在你这个项目上做出一些改动，然后应用到自己的项目中去，这个就是Fork的过程，别人对这个项目的改动，丝毫不会影响你的项目代码组成。 发起请求（Pull Request）​ 这个其实是基于Fork的，别人对你项目做出了修改，想让这个修改后的项目开源，把修改后的内容合并到原有的项目当中去，这个时候他就会发送一个Pull Request（简称PR），原始项目的创建人，也就是你感觉这个修改不错，觉得他的修改很OK，就可以接受他的PR。 关注（Watch）​ 如果你Watch了这个项目，那么以后这个项目有任何的更新，你会第一时间收到这个更新通知。 事务卡片（Issue）​ 发现代码bug，但是目前没有成型的代码，需要讨论的时候用。意思是说，当你开源了一个代码，但是别人发现了你的代码中有bug，或者哪些方面做的不够好，那么他就可以给你一个Issue，问题多了就变成了Issues，你看到这些Issues时，去修复就行。]]></content>
      <categories>
        <category>GitHub</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python3下使用requests实现模拟用户登录]]></title>
    <url>%2F2018%2F08%2F07%2Fpython3%E4%B8%8B%E4%BD%BF%E7%94%A8requests%E5%AE%9E%E7%8E%B0%E6%A8%A1%E6%8B%9F%E7%94%A8%E6%88%B7%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[模拟登录马蜂窝网站提取到的请求信息12345678910Headers： Request URL:https://passport.mafengwo.cn/login/ Request Method:POST origin:https://passport.mafengwo.cn referer:https://passport.mafengwo.cn/ User-Agent:Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 Form Data: passport:13725168940 password:aaa00000000 模拟登录1234567891011121314151617181920212223242526import requestsuserAgent = "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36"header = &#123; # "origin": "https://passport.mafengwo.cn", "Referer": "https://passport.mafengwo.cn/", 'User-Agent': userAgent,&#125;def mafengwoLogin(account, password): # 马蜂窝模仿 登录 print ("开始模拟登录马蜂窝") postUrl = "https://passport.mafengwo.cn/login/" postData = &#123; "passport": account, "password": password, &#125; responseRes = requests.post(postUrl, data = postData, headers = header) # 无论是否登录成功，状态码一般都是 statusCode = 200 print(f"statusCode = &#123;responseRes.status_code&#125;") print(f"text = &#123;responseRes.text&#125;")if __name__ == "__main__": # 从返回结果来看，有登录成功 mafengwoLogin("13756567832", "000000001") 使用cookie访问站点在上一步，我们已经成功登录到马蜂窝网站了。那么接下来要如何访问站点中其他页面呢。前面提到过，网站是通过cookie和session来标记是哪个用户访问的。所以，在我们登录成功之后，有很重要的一步，就是我们需要把cookie保存下来，下一次请求这个站点的页面时，把这个cookie带过去。 保存cookie信息123456789101112131415161718192021222324252627282930313233343536373839404142434445import requests# python2 和 python3的兼容代码try: # python2 中 import cookielib print(f"user cookielib in python2.")except: # python3 中 import http.cookiejar as cookielib print(f"user cookielib in python3.")# session代表某一次连接mafengwoSession = requests.session()# 因为原始的session.cookies 没有save()方法，所以需要用到cookielib中的方法LWPCookieJar，这个类实例化的cookie对象，就可以直接调用save方法。mafengwoSession.cookies = cookielib.LWPCookieJar(filename = "mafengwoCookies.txt")userAgent = "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36"header = &#123; # "origin": "https://passport.mafengwo.cn", "Referer": "https://passport.mafengwo.cn/", 'User-Agent': userAgent,&#125;def mafengwoLogin(account, password): # 马蜂窝模仿 登录 print("开始模拟登录马蜂窝") postUrl = "https://passport.mafengwo.cn/login/" postData = &#123; "passport": account, "password": password, &#125; # 使用session直接post请求 responseRes = mafengwoSession.post(postUrl, data = postData, headers = header) # 无论是否登录成功，状态码一般都是 statusCode = 200 print(f"statusCode = &#123;responseRes.status_code&#125;") print(f"text = &#123;responseRes.text&#125;") # 登录成功之后，将cookie保存在本地文件中，好处是，以后再去获取马蜂窝首页的时候，就不需要再走mafengwoLogin的流程了，因为已经从文件中拿到cookie了 mafengwoSession.cookies.save()if __name__ == "__main__": # 从返回结果来看，有登录成功 # mafengwoLogin("13756567832", "000000001") cookie保存结果如下：123456#LWP-Cookies-2.0Set-Cookie3: __today_login=1; path=&quot;/&quot;; domain=&quot;.mafengwo.cn&quot;; path_spec; domain_dot; expires=&quot;2018-03-16 15:56:15Z&quot;; httponly=None; version=0Set-Cookie3: mafengwo=&quot;0a60e1a04f6a6f5555f0e285602b5b17_94281374_5aab641fb23d42.37804626_5aab641fb23dc3.28763728&quot;; path=&quot;/&quot;; domain=&quot;.mafengwo.cn&quot;; path_spec; domain_dot; expires=&quot;2018-06-13 06:25:03Z&quot;; httponly=None; version=0Set-Cookie3: mfw_uuid=&quot;5aab641f-b789-96ef-736d-48640285f4c0&quot;; path=&quot;/&quot;; domain=&quot;.mafengwo.cn&quot;; path_spec; domain_dot; expires=&quot;2019-03-16 06:25:03Z&quot;; version=0Set-Cookie3: oad_n=&quot;a%3A3%3A%7Bs%3A3%3A%22oid%22%3Bi%3A1029%3Bs%3A2%3A%22dm%22%3Bs%3A20%3A%22passport.mafengwo.cn%22%3Bs%3A2%3A%22ft%22%3Bs%00009%3A%222018-03-16+14%3A28%3A47%22%3B%7D&quot;; path=&quot;/&quot;; domain=&quot;.mafengwo.cn&quot;; path_spec; domain_dot; expires=&quot;2018-03-23 06:25:03Z&quot;; version=0Set-Cookie3: uol_throttle=94281374; path=&quot;/&quot;; domain=&quot;.mafengwo.cn&quot;; path_spec; domain_dot; expires=&quot;2018-03-16 06:35:03Z&quot;; version=0 使用cookie登录为了测试访问页面时，是否处于登录状态。有一个比较巧妙的方法：就是直接访问一个需要登录后，才可见的地址。比如说涉及到用户信息的页面。下面以 “我的路线” 页面为例：http://www.mafengwo.cn/plan/route.php 判断cookie登录是否成功12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import requests# python2 和 python3的兼容代码try: # python2 中 import cookielib print(f"user cookielib in python2.")except: # python3 中 import http.cookiejar as cookielib print(f"user cookielib in python3.")# session代表某一次连接mafengwoSession = requests.session()# 因为原始的session.cookies 没有save()方法，所以需要用到cookielib中的方法LWPCookieJar，这个类实例化的cookie对象，就可以直接调用save方法。mafengwoSession.cookies = cookielib.LWPCookieJar(filename = "mafengwoCookies.txt")userAgent = "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36"header = &#123; # "origin": "https://passport.mafengwo.cn", "Referer": "https://passport.mafengwo.cn/", 'User-Agent': userAgent,&#125;def isLoginStatus(): # 通过访问个人中心页面的返回状态码来判断是否为登录状态 routeUrl = "http://www.mafengwo.cn/plan/route.php" # 下面有两个关键点 # 第一个是header，如果不设置，会返回500的错误 # 第二个是allow_redirects，如果不设置，session访问时，服务器返回302， # 然后session会自动重定向到登录页面，获取到登录页面之后，变成200的状态码 # allow_redirects = False 就是不允许重定向 responseRes = mafengwoSession.get(routeUrl, headers = header, allow_redirects = False) print(f"isLoginStatus = &#123;responseRes.status_code&#125;") if responseRes.status_code != 200: return False else: return Trueif __name__ == "__main__": mafengwoSession.cookies.load() isLogin = isLoginStatus() print(f"is login mafengwo = &#123;isLogin&#125;") ''' # 按照之前保存过的mafengwoCookies.txt登录，属于登录状态： user cookielib in python3. isLoginStatus = 200 is login mafengwo = True ''' ''' # 如果把mafengwoCookies.txt中的信息修改掉之后，就无法登录了，属于非登录状态了 user cookielib in python3. isLoginStatus = 302 is login mafengwo = False ''' 最终形成的登录模式12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import requests# python2 和 python3的兼容代码try: # python2 中 import cookielib print(f"user cookielib in python2.")except: # python3 中 import http.cookiejar as cookielib print(f"user cookielib in python3.")# session代表某一次连接mafengwoSession = requests.session()# 因为原始的session.cookies 没有save()方法，所以需要用到cookielib中的方法LWPCookieJar，这个类实例化的cookie对象，就可以直接调用save方法。mafengwoSession.cookies = cookielib.LWPCookieJar(filename = "mafengwoCookies.txt")userAgent = "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36"header = &#123; # "origin": "https://passport.mafengwo.cn", "Referer": "https://passport.mafengwo.cn/", 'User-Agent': userAgent,&#125;# 马蜂窝模仿 登录def mafengwoLogin(account, password): print("开始模拟登录马蜂窝") postUrl = "https://passport.mafengwo.cn/login/" postData = &#123; "passport": account, "password": password, &#125; # 使用session直接post请求 responseRes = mafengwoSession.post(postUrl, data = postData, headers = header) # 无论是否登录成功，状态码一般都是 statusCode = 200 print(f"statusCode = &#123;responseRes.status_code&#125;") print(f"text = &#123;responseRes.text&#125;") # 登录成功之后，将cookie保存在本地文件中，好处是，以后再去获取马蜂窝首页的时候，就不需要再走mafengwoLogin的流程了，因为已经从文件中拿到cookie了 mafengwoSession.cookies.save()# 通过访问个人中心页面的返回状态码来判断是否为登录状态def isLoginStatus(): routeUrl = "http://www.mafengwo.cn/plan/route.php" # 下面有两个关键点 # 第一个是header，如果不设置，会返回500的错误 # 第二个是allow_redirects，如果不设置，session访问时，服务器返回302， # 然后session会自动重定向到登录页面，获取到登录页面之后，变成200的状态码 # allow_redirects = False 就是不允许重定向 responseRes = mafengwoSession.get(routeUrl, headers = header, allow_redirects = False) print(f"isLoginStatus = &#123;responseRes.status_code&#125;") if responseRes.status_code != 200: return False else: return Trueif __name__ == "__main__": # 第一步：尝试使用已有的cookie登录 mafengwoSession.cookies.load() isLogin = isLoginStatus() print(f"is login mafengwo = &#123;isLogin&#125;") if isLogin == False: # 第二步：如果cookie已经失效了，那就尝试用帐号登录 print(f"cookie失效，用户重新登录...") mafengwoLogin("13756567832", "000000001") resp = mafengwoSession.get("http://www.mafengwo.cn/plan/fav_type.php", headers = header, allow_redirects = False) print(f"resp.status = &#123;resp.status_code&#125;") 1234567891011# 第一次运行程序的输出：# 由于第一次还没有生成cookie，所以需要用账户登录一次user cookielib in python3.isLoginStatus = 302is login mafengwo = Falsecookie失效，用户重新登录...开始模拟登录马蜂窝statusCode = 200……………………resp.status = 200 1234567# 第二次运行程序的输出：# 第二次，就直接使用cookie登录了，不再需要使用帐号登录user cookielib in python3.isLoginStatus = 200is login mafengwo = Trueresp.status = 200]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Network Crawler</tag>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[极验验证码识别与破解]]></title>
    <url>%2F2018%2F08%2F02%2F%E6%9E%81%E9%AA%8C%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB%E4%B8%8E%E7%A0%B4%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[极验验证码极验验证码安全性高，利用机器学习和神经网络，构建线上线下多重静态、动态防御模型。识别模拟轨迹，界定人机边界。 破解思路如果我们直接模拟表单提交，加密参数的构造是个问题，需要分析其加密和校验逻辑，相当繁琐。所以我们直接模拟浏览器动作的方法来完成验证。故我们直接选择应用Selenium来模拟人的行为的方式来完成验证。 故我们先找到带有极验验证的网页，如极验的官方后台，登陆验证按钮如图所示 所以验证步骤为三步（1）模拟点击验证按钮。（2）识别滑动缺口的位置。（3）模拟拖动滑块 第一步直接用应用Selenium来模拟人的行为的方式来完成第二步利用像素差识别缺口的位置和形状，我们可以构建一个边缘检测算法找出缺口的位置，对于极验，我们可以利用原图对比法识别缺口的位置，对于2张图片我们可以设定一个对比阈值然后遍历2张图片，找到同位置像素RGB差距超过此阈值的像素点，那么这个像素点的位置就是缺口的所在处。 第（3）步操作看似简单，但其中的坑比较多。极验验证码增加了机器轨迹识别，匀速移动、随机速度移动等方法都不能通过验证，只有完全模拟人的移动轨迹才可以通过验证。人的移动轨迹一般是先加速后减速，我们需要模拟这个过程才能成功。 有了基本的思路之后，我们就用程序来实现极验验证码的识别过程吧。 初始化选定的链接为https:account.geetest.comlogin也就是极验的管理后台登录页面。在这里我们首先初始化一些配置，如 Selenium对象的初始化及一些参数的配置，如下所示： 12345678EMAIL ='1604784849@qq.com'PASSWORD = 'nicaibudao'class CrackGeetest(): def __init__(self): self.url = 'https:account.geetest.com/login' self.browser = webDriver.Chrome() self.wait = WebDriverWait(self.browser,20) self.password = PASSWORD EMAIL和PASSWORD就是登陆校验的账号和密码，要事先注册。 模拟点击利用显示等待获取这个验证按钮123def get_geetest_button(self): button = self.wait.until(EC.element_to_be_clickable(By.CLASS_NAME,'geetest_radar_tip')) return button 获取一个WebElement对象，调用它的click()，即可模拟点击，代码如下：12button=self.get_geetest_button() button.click() 第一步工作完成。 识别缺口接下来识别缺口的位置。首先获取前后两张比对图片，二者不一致的地方即为缺口。获取不带缺口的图片，利用Selenium选取图片元素，得到其所在位置和宽高，然后获取整个网页的截图，图片截切出来即可，代码实现如下 12345678910111213def get_position(self): img = self.wait.until(EC.presence_of_element_located(By.CLASS_NAME,'geetest_canvas_img')) time.sleep(2) location = img.location size=img.size top,bottom,left,right = location['y'],location['y']+size['height'],location['x'],location['x']+size['width'] return (top,bottom,left,right)def get_geetest_image(self, name='captcha.png'): top, bottom, left, right = self.get_position() print('验证码位置', top, bottom, left, right) screenshot = self.get_screenshot() captcha = screenshot.crop((left, top, right, bottom)) return captcha 这里get_position()函数首先获取图片对象，获取它的位置和宽高，随后返回其左上角和右下角的坐标。get_geetest image()方法获取网页截图，调用了crop()方法将图片裁切出来，返回的是Image对象 接下来我们需要获取第二张图片，也就是带缺口的图片。要使得图片出现缺口，只需要点击下方的滑块即可。这个动作触发之后，图片中的缺口就会显现，如下所示 123def get_slider(self): slider = self.wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'geetest_slider_button'))) return slider 这里利用get_Slider()方法获取滑块对象，调用Click()方法即可触发点击，缺口图片即可呈现，如下所示 12slider = self.get_slider()slider.click() 调用 get_geetest_image()方法将第二张图片获取下来即可。 现在我们已经得到两张图片对象，分别赋值给变量 Image1和 1mage2。接下来对比图片获取缺口。我们在这里遍历图片的每个坐标点，获取两张图片对应像素点的RGB数据。如果二者的RGB数据差距在一定范围内，那就代表两个像素相同，继续比对下一个像素点。如果差距超过一定范围，则代表像素点不同，当前位置即为缺口的位置，代码实现如下： 1234567891011121314151617181920212223242526272829303132def is_pixel_equal(self, image1, image2, x, y): """ 判断两个像素是否相同 :param image1: 图片1 :param image2: 图片2 :param x: 位置x :param y: 位置y :return: 像素是否相同 """ # 取两个图片的像素点 pixel1 = image1.load()[x, y] pixel2 = image2.load()[x, y] threshold = 60 if abs(pixel1[0] - pixel2[0]) &lt; threshold and abs(pixel1[1] - pixel2[1]) &lt; threshold and abs( pixel1[2] - pixel2[2]) &lt; threshold: return True else: return Falsedef get_gap(self, image1, image2): """ 获取缺口偏移量 :param image1: 不带缺口图片 :param image2: 带缺口图片 :return: """ left = 60 for i in range(left, image1.size[0]): for j in range(image1.size[1]): if not self.is_pixel_equal(image1, image2, i, j): left = i return left return left get_gap()方法即取缺日位置的方法：此方法的参数是两张图片、一张为带缺口图片，另一张为像素是否相同。比较两张图RGB的绝对值是否均小于定义的值threshold。如果绝对值均在值之内，则代表像素点相同．継遍历。否则代表不相同的像素点、即缺口的位置。 两张图片有两健明显不同的地方：一个就是待拼合的滑块，一个就是缺口。滑块的位置会出现在左边位置，缺口会出現在与滑块同一水平线的位置，所以缺口一般会在滑块的右側。如果要寻找缺口，直接从滑块右侧寻找即可。我们直接设置遍历的起始横坐标为60，也就是从滑块的右侧开始识别，这样识别出的结果就是缺口的位置。 现在，我们获取了缺口的位置。完成验证还剩下最后一步一步模拟拖动。 模拟拖动模拟拖动过程不复杂，但其中的坑比较多。现在我们只需要调用拖动的相关函数将滑块拖动到对应位置，如果是匀速拖动，极验必然会识别出它是程序的操作，因为人无法做到完全匀速拖动。极验验证码利用机器学习模型，此类数据为机器操作，验证码识别失败。 我们尝试分段模拟，将拖动过程划分几段，每段设置一个平均速度，速度围绕该平均速度小幅度随机抖动，这样也无法完成验证。 最后，完全模拟加速减速的过程通过了验证。前段滑块做匀加速运动，后段滑块做匀减速运动，利用物理学的加速度公式即可完成验证。 滑块滑动的加速度用a来表示，当前速度用v表示，初速度用vo表示，位移用x表示，所需时间用t表示，它们之间满足如下关系： x = v0 t + 0.5 a a a v = v0 + a * t 利用这两个公式可以构造轨迹移动算法，计算出先加速后减速的运动轨迹，代码实现如下所示 1234567891011121314151617181920212223242526272829303132333435def get_track(self, distance): """ 根据偏移量获取移动轨迹 :param distance: 偏移量 :return: 移动轨迹 """ # 移动轨迹 track = [] # 当前位移 current = 0 # 减速阈值 mid = distance * 4 / 5 # 计算间隔 t = 0.2 # 初速度 v = 0 while current &lt; distance: if current &lt; mid: # 加速度为正2 a = 2 else: # 加速度为负3 a = -3 # 初速度v0 v0 = v # 当前速度v = v0 + at v = v0 + a * t # 移动距离x = v0t + 1/2 * a * t^2 move = v0 * t + 1 / 2 * a * t * t # 当前位移 current += move # 加入轨迹 track.append(round(move)) return track track是一个列表，列表的每个元素代表每次移动多少距离。 首先定义变量mid，即减速的阈值，也就是加速到什么位置开始减速。在这里mid值为4/5。即模拟前45路程是加速过程，后1/5路程是减速过程。 接着定义当前位移的距离变量 current，初始为0，然后进入 while循环，循环的条件是当前位移小于总距离。在循环里我们分段定义了加速度，其中加速过程的加速度定义为2，减速过程的加速度定义为－3。之后套用位移公式计算出某个时间段内的位移，将当前位移更新并记录到轨迹里即可。 直到运动轨迹达到总距离时，循环终止。最后得到的track记录了每个时间间隔移动了多少位移，这样滑块的运动轨迹就得到了。 最后按照该运动轨迹拖动滑块即可，方法实现如下所示：123456789101112def move_to_gap(self, slider, track): """ 拖动滑块到缺口处 :param slider: 滑块 :param track: 轨迹 :return: """ ActionChains(self.browser).click_and_hold(slider).perform() for x in track: ActionChains(self.browser).move_by_offset(xoffset=x, yoffset=0).perform() time.sleep(0.5) ActionChains(self.browser).release().perform() 这里传的参数为滑块对象和运动轨迹。首先调用Actionchains的clickand hold()方法按住施动底部滑块，遍历运动轨迹获取每小段位移距离，调用move_by_offset()方法移动此位移，最后调用release()方法松开鼠标即可。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Network Crawler</tag>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图形验证码的识别]]></title>
    <url>%2F2018%2F08%2F02%2F%E5%9B%BE%E5%BD%A2%E9%AA%8C%E8%AF%81%E7%A0%81%E7%9A%84%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[图形验证码识别利用OCR技术可以对图形验证码进行识别，要先准备库tesserocr，如果安装出错，那就安装pytesseract库，因为tesserocr主要用于Linux系统，在win系统中有太多的不兼容，那么我们选用谷歌的pytesseract一样可以达到目的 验证码处理12345import pytesseractfrom PIL import Imageimage = Image.open('code.jpg')print(pytesseract.image_to_string(image)) 如果直接利用第三方库肯定识别会有偏差，这是因为验证码旁边多余的线条干扰了图片的识别 图片处理那么我们就必须对图片进行处理，如转灰度、二值化等利用Image对象的convert()的方法参数传入L，即可将图像转化为灰度图像，代码如下： 12image = image.convert('L')image.show() 传入1，对图片进行二值化，代码如下：12image = image.convert('1')image.show() 同时我们还可以指出二值化的阈值，上面采用的默认127阈值，我们不能直接转化原图，要将原图转化为灰度图像，而后在指定二值化阈值，代码如下： 1234567891011image = image.convert('L')threshold = 80table = []for i in range(256): if i &lt; threshold: table.append(0) else: table.append(1)image = image.point(table, '1')image.show() 综合代码如下：12345678910111213141516171819import tesserocrfrom PIL import Imageimage = Image.open('code.jpg')image = image.convert('L')threshold = 127table = []for i in range(256): if i &lt; threshold: table.append(0) else: table.append(1)image = image.point(table, '1')image.show()result = tesserocr.image_to_text(image)print(result) 输出：GTVM]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Network Crawler</tag>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用XPath的特性简单抓取豆瓣电影]]></title>
    <url>%2F2018%2F08%2F01%2F%E5%88%A9%E7%94%A8XPath%E7%9A%84%E7%89%B9%E6%80%A7%E7%AE%80%E5%8D%95%E6%8A%93%E5%8F%96%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%2F</url>
    <content type="text"><![CDATA[今天突然想回顾一下python解析库XPath，利用Xpath的特性可以比正则表达式更快捷的获取网页中的HTML节点，更快的获取网页中的数据。 实例如下:利用requests模拟用户1234567891011import requestsfrom lxml import etreeheaders=&#123; 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36', 'Referer':'https://movie.douban.com/'&#125;url='https://movie.douban.com/cinema/nowplaying/zhengzhou/'response=requests.get(url,headers=headers)text=response.text 利用Xpath的特性获取HTML节点内容1234567891011121314151617181920212223html=etree.HTML(text)ul=html.xpath("//ul[@class='lists']")[0]lis=ul.xpath("./li")for li in lis: title=li.xpath("@data-title")[0] score = li.xpath("@data-score")[0] duration = li.xpath("@data-duration")[0] region=li.xpath("@data-region")[0] director=li.xpath("@data-director")[0] actors=li.xpath("@data-actors")[0] thumbnail=li.xpath(".//img/@src")[0] movies = [] movie=&#123; 'title': title, 'score':score, 'duration':duration, 'region':region, 'director':director, 'actors':actors, 'thumbnail':thumbnail &#125; movies.append(movie)print(movies) 是不是觉得比正则表达式简单许多！！！ 网页HTML节点内容图片]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Network Crawler</tag>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分析Ajax爬取今日头条街拍美图]]></title>
    <url>%2F2018%2F07%2F31%2F%E5%88%86%E6%9E%90Ajax%E7%88%AC%E5%8F%96%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1%E8%A1%97%E6%8B%8D%E7%BE%8E%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[今天初学了Ajax，Ajax技术可以利用JavaScript在网页不被刷新的情况下与服务器交换数据并更新网页的部分技术，于是乎，利用Ajax的这一特性，决定去爬取今日头条的街拍图片并且实现自动分类汇总。 实战演练实现方法get_page()来加载单个的Ajax的请求过程123456789101112131415161718import requestsfrom urllib.parse import urlencodedef get_page(offert): params=&#123; 'offset':offset, 'format':'json', 'keyword':'街拍', 'autoload':'ture', 'count':'20', 'cur_tab':'1' &#125; url='https://www.toutiao.com/search_content/?'+ urlencode(params) try: response=requests.get(url) if response.status_code==200: return response.json() except requests.ConnectionError: return None 这里我们通过网页开发者模式可以看出URL的基本构造，唯一不变的是参数offset，所以我们将他作为参数传递，我们使用urlencode()的方法来实现网页的GET请求，然后用requests实现整个网页的GET请求，如果连接的状态码为200，那么返回为用函数response的json()将结果转化为JSON格式内容。 获取图片并且截获主题标题12345678910def get_image(json): if json.get('data'): for item in json.get('title'): title=item.get('title') images=item.get('image_detail') for image in images: yield &#123; 'image':image.get('url'), 'title':title &#125; 实现一个解析方法：提取每条数据中的”imag_etail”字段中的每一张图片链接，将图片链接和图片所属的标题一并返回，此时可以构造一个生成器。 实现保存图片的方法1234567891011121314151617import osfrom hashlib import md5def save_image(item): if not os.path.exists(item.get('title')): os.mkdir(item.get('title')) try: response=requests.get(item.get('image')) if response.status_code==200: file_path = '&#123;0&#125;/&#123;1&#125;.&#123;2&#125;'.format(item.get('title'),md5(response.content).hexdigest(),'jpg') if not os.path.exists(file_path): with open(file_path,'wb') as f: f.write(response.content) else: print('Already Downloaded',file_path) except requests.ConnectionError: print('Failed to Save Image') 用save_image()函数实现图片的保存，item不过是前面get_image函数返回的一个字典。在该方法中，首先根据item的title来创建一个文件夹，然后请求这个图片链接，获取图片的二进制数据，以二进制的形式写入文件。图片的名称可以使用可以使用其内容的Md5的值，这样可以去除反复 实现多进程下载12345678910111213def main(offset): json = get_page(offset) for item in get_images(json): print(item) save_image(item)if __name__ == '__main__': pool = Pool() groups = ([x * 20 for x in range(GROUP_START, GROUP_END + 1)]) pool.map(main, groups) pool.close() pool.join() 我们只需要构造一个offset数组，遍历offset，提取图片链接并下载，在这里也实现了起始页数和终止页数，还利用了多进程池，调用map()的方法实现多进程下载]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Network Crawler</tag>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用第三方解析库爬取知乎推荐页面并采用txt文本储存]]></title>
    <url>%2F2018%2F07%2F31%2F%E5%88%A9%E7%94%A8%E7%AC%AC%E4%B8%89%E6%96%B9%E8%A7%A3%E6%9E%90%E5%BA%93%E7%88%AC%E5%8F%96%E7%9F%A5%E4%B9%8E%E6%8E%A8%E8%8D%90%E9%A1%B5%E9%9D%A2%E5%B9%B6%E9%87%87%E7%94%A8txt%E6%96%87%E6%9C%AC%E5%82%A8%E5%AD%98%2F</url>
    <content type="text"><![CDATA[采用第三方库快捷爬取知乎推荐页面源代码并解析：123456789101112131415161718import requests from pyquery import PyQuery as pqurl="https://www.zhihu.com/explore"headers =&#123; 'User-Agent':"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36"&#125; #虚拟用户登陆，防止封iphtml=requests.get(url,headers=headers).textdoc=pq(html) #初始化文本items=doc('.explore-tab .feed-item').items()for item in items: question=item.find("h2").text() author=item.find(".author-link-line").text() answer=pq(item.find('.content').html()).text() file=open('explore.txt','a',encoding='utf-8') file.write('\n'.join([question,author,answer])) file.write('\n'+'=' * 10 + '\n') file.close() 文本代码解析这里主要是为了演示文件保存的方式，因此省略requests库的异常处理应用，通过requests库提取知乎发现一栏的网页源代码，然后爬取知乎热门话题中的问题、回答者、答案全部提取出来，利用python中的open打开一个文件，write写入文件，最后用close关闭，内容也就保存在文本中了 效果图如下：]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Network Crawer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫实战（一）爬取猫眼top前100]]></title>
    <url>%2F2018%2F07%2F30%2F%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BCtop%E5%89%8D100%2F</url>
    <content type="text"><![CDATA[爬虫实战（一）利用python爬取猫眼电影Top前100代码及其解析：12345678910111213141516171819202122232425262728293031323334353637383940import requests#爬虫库import json#json数据格式库 from requests.exceptions import RequestException#requests异常 import re#正则表达式 import time #延迟函数 def get_one_page(url):#定义一个读取一个url并返回相应信息的函数try: headers=&#123; "User-Agent":'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36' &#125;#伪装浏览器 response=requests.get(url,headers=headers)#读取网页 if response.status_code==200:#判断是否读取成功 return response.text#返回读取的内容（html代码） return Noneexcept RequestException: return None def parse_one_page(html):#定义一个解析html代码的函数#编译成一个正则表达式对象pattern = re.compile(r'&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a' + '.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p&gt;' + '.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S) items = re.findall(pattern, html)#开始查找 for item in items:#遍历查找到的内容#使用关键字yield 类似于return 返回的是一个生成器对象 yield &#123;'index': item[0], 'image': item[1], 'title': item[2], 'actor': item[3].strip()[3:], 'time': item[4].strip()[5:], 'score': item[5] + item[6] &#125; def write_to_file(content):#将结果写到一个txt文档中 with open('result.txt', 'a', encoding='utf-8') as f: f.write(json.dumps(content, ensure_ascii=False) + '\n') def pic_download(url,title):#图片下载 r=requests.get(url) with open("pics/"+title+".jpg",'wb') as f: f.write(r.content) def main(offset):#打开需要爬取得所有网页，并进行爬取 url='http://maoyan.com/board/4?offset='+str(offset)#网页链接 html=get_one_page(url)#请求网页，获取html for item in parse_one_page(html):#遍历处理后的html结果 pic_download(item['image'], item['title'])#下载图片 write_to_file(item)#写入到文件中if __name__ == '__main__': for i in range(10): main(offset=i*10) time.sleep(1)#延迟1秒，避免反爬机制 ##]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Network Crawler</tag>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的a[::1]类型]]></title>
    <url>%2F2018%2F07%2F30%2FPython%E4%B8%AD%E7%9A%84%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[for value in rang(10)涉及的数字倒序输出： for value in rang(10)[::-1]涉及的数字倒序输出： 一、反转二、详解这个是python的slice notation的特殊用法。 a = [0,1,2,3,4,5,6,7,8,9] b = a[i:j] 表示复制a[i]到a[j-1]，以生成新的list对象 b = a[1:3] 那么，b的内容是 [1,2] 当i缺省时，默认为0，即 a[:3]相当于 a[0:3] 当j缺省时，默认为len(alist), 即a[1:]相当于a[1:10] 当i,j都缺省时，a[:]就相当于完整复制一份a了 b = a[i:j:s]这种格式呢，i,j与上面的一样，但s表示步进，缺省为1. 所以a[i:j:1]相当于a[i:j] 当s&lt;0时，i缺省时，默认为-1. j缺省时，默认为-len(a)-1所以a[::-1]相当于 a[-1:-len(a)-1:-1]，也就是从最后一个元素到第一个元素复制一遍。所以你看到一个倒序的东东。 如果还不理解，把我说的东西测试一遍，你就明白了]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>python Basic Grammar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中__name__的意义以及作用]]></title>
    <url>%2F2018%2F07%2F30%2Fpython%E4%B8%AD-name-%E7%9A%84%E6%84%8F%E4%B9%89%E4%BB%A5%E5%8F%8A%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[首先定义了一个test.py的文件，然后再定义一个函数，并在函数定义后直接运行： test.py def HaveFun(): if __name__ == &apos;__main__&apos;: print(&apos;I am in my domain,my name is %s&apos; % __name__) else: print(&apos;Someone else calls me!,my name is %s&apos; % __name__) HaveFun() 运行test.py结果： I am in my domain,my name is __main__ 然后继续创建一个main.py的文件，程序如下： main.py import test test.HaveFun() 执行main.py文件，结果如下： Someone else calls me!,my name is test Someone else calls me!,my name is test 这里打印了两次，第一次实在main.py在进行import test的时候，进行的打印，第二次才是test.HaveFun()中执行的打印，可以发现，这里的 name名称已经发成了变化，从之前的main变成了模块名称test。 总结：1、__name__这个系统变量显示了当前模块执行过程中的名称，如果当前程序运行在这个模块中，__name__ 的名称就是__main__如果不是，则为这个模块的名称。 2、__main__一般作为函数的入口，类似于C语言，尤其在大型工程中，常常有if __name__ == &quot;__main__&quot;:来表明整个工程开始运行的入口。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>python Basic Grammar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux中如何开启root]]></title>
    <url>%2F2018%2F07%2F30%2Flinux%E4%B8%ADroot%E6%A8%A1%E5%BC%8F%E5%BC%80%E5%90%AF%2F</url>
    <content type="text"><![CDATA[sudo passwd root, 为root输入新密码， 然后输入su root 回车后， 输入刚才新建的root密码， 就进入了root用户。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pycharm的快捷操作汇总]]></title>
    <url>%2F2018%2F07%2F30%2Fpycharm%E7%9A%84%E5%BF%AB%E6%8D%B7%E6%93%8D%E4%BD%9C%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[Alt+Enter 自动添加包 shift+O 自动建议代码补全 Ctrl+t SVN更新 Ctrl+k SVN提交 Ctrl + / 注释(取消注释)选择的行 Ctrl+Shift+F 高级查找 Ctrl+Enter 补全 Shift + Enter 开始新行 TAB Shift+TAB 缩进/取消缩进所选择的行 Ctrl + Alt + I 自动缩进行 Ctrl + Y 删除当前插入符所在的行 Ctrl + D 复制当前行、或者选择的块 Ctrl + Shift + J 合并行 Ctrl + Shift + V 从最近的缓存区里粘贴 Ctrl + Delete 删除到字符结尾 Ctrl + Backspace 删除到字符的开始 Ctrl + NumPad+/- 展开或者收缩代码块 Ctrl + Shift + NumPad+ 展开所有的代码块 Ctrl + Shift + NumPad- 收缩所有的代码块]]></content>
      <categories>
        <category>Python3</category>
        <category>Pycharm</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pycharm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python解析库XPath]]></title>
    <url>%2F2018%2F07%2F29%2FXpath%2F</url>
    <content type="text"><![CDATA[Python解析库（一）：使用XPath表达式 描述nodename 选取此节点的所有子节点 / 从当前节点选取直接子节点 // 从当前节点选取子孙节点 . 选取当前节点 . . 选取当前节点的父节点 @ 选取属性 例如： //title[@lang=’eng’] 这是一个XPath规则，他代表选取所有名称为title，同时属性为lang的值为ang的节点。 2.关于lxml库中的etree模块：先用etree模块申明一段HTML文本，etree模块可自动补全HTML文本， 然后调用tostring（）方法输出修正后的HTML文本，结果为bytes型 利用decode（）方法转化为str类型 3.所有节点：用//开头选取所有符合要求的节点，声明文本后，调用xpath（）方法，例如：xpath（//节点名称） 返回的为列表，其中的每一个元素都是一个Element对象，可选择中括号加索引的形式取出其中的一个对象 4.子节点：通过//和/来获取当前节点的子节点或者子孙节点； /用来获取直接子节点，例如result=html.xpath（//li/a） //用来获取子孙节点，例如result=html.xpath（//ul//a） 5.父节点：用. .来查找父节点 比如： from lxml import etree html = etree.parse(&apos;./text.html&apos; , etree.HTMLParser()) result = html.xpath(&apos;//a[@href=&quot;link4.html&quot;]/. ./@class&apos;] print(result) 当然，. .可以用parent::来替换 6.属性匹配：比如要选取class为item-0的li节点，可以这样实现： from lxml import etree html=etree.parse(&apos;./text.html&apos;,etree.HTMLParser()) result = html.xpath(&apos;//li[@class=&apos;items-0&apos;]&apos;) print(result) 7.文本获取：可以利用XPath中的text（）方法获取其中节点中的文本 选取相应的节点获取文本 from lxml import etree html = etree.parse(&apos;./text.html&apos; , etree.HTMLParser()) result = html.xpath(&apos;//li[@class=&quot;item-0&quot;]/a/text()&apos;) print(result) 利用//来实现选取结果 from lxml import etree html = etree.parse(&apos;./text.html&apos; , etree.HTMLParser()) result = html.xpath(&apos;//li[@class=&quot;item-0&quot;]/a/text()&apos;) print(result) 8.按序选择：在li节点的括号中传入数字1即可，这里的序号是以1开头，不是0 例如：result = html.xpath(&apos;//li[1]/a/text()&apos;) 在括号中传入last（）可知，获取的便是最后一个li节点 例如：result = html.xpath(&apos;//li[last()]/a/text()&apos;) 选取位置小于3的li节点，利用position（）函数实现 例如：result = html.xpath(&apos;//li[position()&lt;3]/a/text()&apos;) 获取倒数第三个li节点：用last()实现 例如：result = html.xpath(&apos;//li[last()-2]/a/text()&apos;) 9.节点轴的选取：节点的轴方法，可获取子元素，兄弟元素，父元素，祖先元素等 result = html.xpath(&apos;//li[1]/ancestor::*&apos;) #ancestor轴，用于获取所有祖先元素 result = html.xpath(&apos;//li[1]/ancestor::div&apos;)#加限定条件div result = html.xpath(&apos;//li[1]/child::a[href=&quot;link1.html&quot;]&apos;)#获取相应属性的子节点 result = html.xpath(&apos;//li[1]/descendant::span&apos;)#descendant获取所有的子孙节点，span是限定条件 result = html.xpath(&apos;//li[1]/following::*[2]&apos;)#following轴，获取当前节点之后的所有节点，但是这里加了索引限制 result = html.xpath(&apos;//li[1]/following-sibling::*&apos;)#follow-sibling轴，可以获取当前节点之后的所有同级节点]]></content>
      <categories>
        <category>Python3</category>
        <category>Network Crawler</category>
      </categories>
      <tags>
        <tag>Network Crawler</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F07%2F29%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
